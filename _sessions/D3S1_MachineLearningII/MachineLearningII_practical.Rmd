---
title: "Machine Learning II"
author: "BaselRBootcamp January 2018"
output: html_document
---

```{r, echo = FALSE, fig.align = 'center', out.width = "50%", fig.cap = "Source: https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer"}
knitr::include_graphics("https://uploads.toptal.io/blog/image/443/toptal-blog-image-1407508081138.png")
```


```{r, echo = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, fig.align = 'center')
```


### Slides

- [Here are the introduction slides for this practical on machine learning!](https://therbootcamp.github.io/_sessions/D2S3_MachineLearning/MachineLearning.html)


### Overview

In this practical you'll use the `caret` (**C**lassification **A**nd **RE**gression **T**raining) package to automate many aspects of the machine learning process. By the end of this practical you will know:

1. Which aspects of the machine learning process the `caret` package can automate
2. Select different 'learners' available in the `caret` package
3. Fit parameters of a learner to a dataset
4. Explore learners to understand how they work
5. Use cross validation techniques to estimate the accuracy of a learner
6. Make a prediction for a real dataset!

### Glossary and packages

Here are the main functions and packages you'll be using. For more information about the specific models, click on the link in *Additional Details*.

| Algorithm| Function| Package | Additional Details |
|:------|:--------|:----|:----|
|     Regression|    `glm()`| Base R| https://bookdown.org/ndphillips/YaRrr/regression.html#the-linear-model|
|     Fast-and-Frugal decision trees|    `FFTrees()`| FFTrees| https://cran.r-project.org/web/packages/FFTrees/vignettes/guide.html|
|     Decision Trees|    `rpart()`| `rpart` | https://statweb.stanford.edu/~lpekelis/talks/13_datafest_cart_talk.pdf|
| Random Forests | `randomForest()` | `randomForest` | http://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/|

### Packages

For this practical, you will need the following packages:

```{r, eval = FALSE}
# Install all of the packages necessary for this practical
install.packages(c("caret", "e1071", "doSNOW", "ipred", "xgboost", "mlbench", "rpart", "mlr"))
```



```{r, echo = FALSE, eval = FALSE}
library(caret)


iris_train_s <- createDataPartition(iris$Species, times = 1, p = .5)

iris_train <- iris[]

```


### Examples

- The following examples will take you through all steps of the machine learning process, from creating training and test data, to fitting models, to making predictions. Follow along and try to see how piece of code works!

```{r, eval = FALSE, echo = TRUE}
# -----------------------
# Part A:
# Load libraries
# -----------------------
library(caret)          # for randomForest()
library(mlr)        # for FFTrees and the heartdisease data
library(mlbench)      # for dplyr and ggplot2
library(GGally)   # For ggpairs

# -----------------------
# Part B: Load training dataset
# Look at https://www.kaggle.com/c/titanic/data for a full description
# -----------------------

titanic_full <- read_csv("https://raw.githubusercontent.com/pcsanwald/kaggle-titanic/master/train.csv")

# Create training and test data

train_cases <- createDataPartition(y = titanic_full$survived,
                                   times = 1)

titani_train <- titanic_full %>%
  slice(train_cases$Resample1)


#=================================================================
# Impute Missing Ages
#=================================================================

# Explore columns statistics with mlr::summarizeColumns
titanic_full %>%
  mlr::summarizeColumns()







# Caret supports a number of mechanism for imputing (i.e., 
# predicting) missing values. Leverage bagged decision trees
# to impute missing values for the Age feature.

# First, transform all feature to dummy variables.
dummy_vars <- dummyVars(~ ., data = train %>% select(-survived))
train_dummy <- predict(dummy_vars, newdata = train %>% select(-survived))

# 
train_bagImpute_model <- preProcess(train_dummy, method = "bagImpute")
train_inputed <- as_tibble(predict(train_bagImpute_model, newdata = train_dummy))

#=================================================================
# Split Data
#=================================================================

# Use caret to create a 70/30% split of the training data,
# keeping the proportions of the Survived class label the
# same across splits.
set.seed(100)

train_indices <- createDataPartition(train_inputed$survived,
                                     times = 3,
                                     p = 0.7,
                                     list = FALSE)

titanic_train <- train_inputed %>% 
  slice(train_indices[,1])

titanic_test <- train_inputed %>% 
  slice(-train_indices[,1])


# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(train$survived))
prop.table(table(titanic_train$survived))
prop.table(table(titanic_test$survived))

str(train)


#=================================================================
# Train Model
#=================================================================

# Set up caret to perform 10-fold cross validation repeated 3 
# times and to use a grid search for optimal model hyperparamter
# values.
train.control <- trainControl(method = "repeatedcv",
                              number = 10,              # 10-fold 
                              repeats = 3,              # Do it 3 times
                              search = "grid")          # Find optimal parameters

caret.cv <- train(survived ~ ., 
                  data = titanic_train,
                  method = "xgbTree",
                  trControl = train.control)






```


### Datasets

We will use two main datasets in this practical, `boston` and `breastcancer`. The Boston dastaset looks like this:

```{r}
DT::datatable(BostonHousing)
```


crim	per capita crime rate by town
zn	proportion of residential land zoned for lots over 25,000 sq.ft
indus	proportion of non-retail business acres per town
chas	Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
nox	nitric oxides concentration (parts per 10 million)
rm	average number of rooms per dwelling
age	proportion of owner-occupied units built prior to 1940
dis	weighted distances to five Boston employment centres
rad	index of accessibility to radial highways
tax	full-value property-tax rate per USD 10,000
ptratio	pupil-teacher ratio by town
b	1000(B - 0.63)^2 where B is the proportion of blacks by town
lstat	percentage of lower status of the population
medv	median value of owner-occupied homes in USD 1000's

## (E)xplore

1. Load the training data into R as a new object with the name `boston_train` using `read_csv()`.

```{r, eval = FALSE}
boston_train <- read_csv("data/boston_train.csv")
```

2. Explore the data using the standard functions `head()`, `View()`, `names()` and `summary()`

3. Use the `summarizeColumns()` function from the `mlr` package to create numerical summaries of each column. Do you notice any strange columns?

```{r}
mlr::summarizeColumns(boston_train)
```

4. Can you think of a reason why you should remove any of the columns? If so, remove them using `select()`! There's no need to include unnecesary data in a machine learning algorithm :)

5. Visualise the data using the `ggpairs()` function from the `GGally` package. If your criterion is categorical, consider including coloring the results by the criterion by including the argument `aes(col = y)` (where y is the name of the criterion). Do you notice any interesting patterns?

```{r}
ggpairs(boston_train)
```

### (T)rain

6. We will use 10-fold cross validation to train the models. To set up the fitting process, we'll need to use the `trainControl` function. Look at the help menu for this function and look through the examples.

7. Using the `trainControl()` function, create an object called `rep10_control` that will conduct 10-fold cross validation.

```{r}
rep10_control <- caret::trainControl(method = "repeatedcv",
                                    number = 10,
                                    search = "grid")          # Find optimal parameters
```

8. We're almost ready to train models! To do so, we'll use the almighty `train()` function. Look at the help menu for the `train()` function to see all of its lovely arguments and examples

9. Now it's time to actually train a model We'll start with simple linear regression. Create an object called `boston_lm_train`  where you predict the criterion. In order to conduct linear regresion, include the arguments `method = 'lm'` and `trControl = rep10_control`

```{r}
boston_regression_train <- caret::train(form = crim ~ ., 
                                        data = boston_train,
                                        method = "ridge",
                                        trControl = rep10_control)
```

10. Explor your `boston_lm_train` argument using the generic functions `names()`, `summary()`, and `plot()`. What is stored in this object?

11. Using the `varImp()` function, look at the variable importance of each predictor. Which predictors seem to be the most important? After you've run the function, try plotting the object with `plot()` to visualise the results!

```{r}
caret::varImp(boston_regression_train)
```

12. Load the test dataset as a new object called `boston_test` into R using `read_csv()`

13. Now it's time to make predictions form your model! Using `predict()`, predict the criterion values for the test dataset `boston_test`. In the `object` argument, use your `boston_lm_train` model, in the `newdata` argument, use your `boston_test` test data. Save the results as the vector `boston_lm_predictions`

```{r}
boston_lm_predictions <- predict(boston_lm_train, 
                                 newdata = boston_test)
```

### (E)valuating model performance

14. How well did your model do in predicting the true data? To start, plot the relationship between your model predictions and the true criterion using the following template

```{r, eval = FALSE}
# Combine predictions and criterion in one tibble
performance_data <- tibble(predictions = boston_regression_predictions,
                          criterion = boston_test$crim)

# Plot results
ggplot(data = performance_data,
       aes(x = predictions, y = criterion)) +
  geom_point() +   # Add points
  geom_abline(slope = 1, intercept = 0, col = "blue", size = 2) +
  labs(title = "Regression prediction accuracy",
       subtitle = "Blue line is perfect prediction!")
```

15. Now it's time to quantify how well your model did. To do this, we'll evaluate its performance with `postResample()`. Start by looking at the help menu for the `postResample()` function to see its arguments.

16. Now evaluate the model's predictions using `postResample()`. You should only specify the `pred` and `obs` arguments as `pred = boston_regression_predictions` and `obs = boston_test$crim`. 

```{r}
postResample(pred = X_regression_predictions, 
             obs = boston_test$crim)
```

17. How well did your model do? What was the overall level of accuracy? If you don't understand the results, look at the details in the help menu for help!

## Selecting different models

18. Now let's try a new model! Instead of using simple linear regression with `method = 'lm'`, we'll use so--called 'ridge regression'. Ridge regression is a variation of 'standard' linear regression that tries to avoid overfitting and thus make better predictions. How can you use ridge regression in caret? Try to answer yourself by looking at the list of all available models on the caret help site: [http://topepo.github.io/caret/available-models.html](http://topepo.github.io/caret/available-models.html). Search for 'ridge' and find the `method` value.

19. Repeat steps 9 through 16 to create a *new* set of objects using ridge regression instead of standard linear regression.  When creating your objects, you mauy want to use the name `_ridge_` instead of `_regression_` so you know which object is which!. As you are creating your new objects, make sure to stop and explore them (i.e.; with `summary()`, `plot()`, `print()` to see how they look different from the previous objects. 

20. Which model predicted the data better? Standard regression or ridge regression? Was using ridge regression able to improve your predictions?

21. Now you're getting the hang of it! Instead of regression, let's try a totally different model....random forests! Repeat all of your steps to see how well random forests do in predicting the `boston_test` data.










 





### BostonHousing

1. Look at the help menu for your dataset to understand the differnt columns ("features") in the dataset. Make a mental note of which column is the criterion! Is the criterion numeric, binary, or factor?

2. Visually explore the dataset using `summary()`, `str()` and `View()`

3. Using `createPartition`, split the original dataset into a 70\% training set, and a 30\% test set. Make sure to stratify the data according to the criterion.

4. Using `map_dfc`, look for missing data in the training data. Do you find any?

```{r, eval = FALSE}
# Look for missing data in each column of data_train

data_train %>%
  map_dfc(~ sum(is.na(.)))
```

5. If you found mising data, create a model of the missing values using `preProcess`. Then create a new training dataset containing no missing values using `predict()`

6. Create a set of control parameters to setup a 10 fold cross validation.

7. Using `train`, train 3 different models on your data: 
  - Decision tree,
  - Regression (or logistic regression for a binary predictor)
  - Support vector machines

8. Look at the results of the object you just created. Use `names()`, `str()` and `summary()` to see what's in the objects. See if you can access each of the 10 models for each fold of the cross validation.

9. Using XXX, look at the accuracy results, what were the 10 different prediction accuracy levels you got in the 10 fold cross validation? Based on these results, which model do you think will do the best in predicting the true test data?

10. Use XXX to visualise the prediction results for each model.

11. Now, using XX predict the criterion for the *real* test data.

12. Using XX, evaluate the true test accuracy of the model. How did it compare to what you expected from your 10 fold cross validation simulation?

13. Using XX, visualise the final prediction results wi

### Additional options

12. Instead of creating a 70 / 30 training / test split, where 70% of the data were used for training and 30% were used for testing, try repeating your anlyses with a 95 / 5 split. Do your results from before hold?

### Advanced

### Optimising parameters with cross validation.

13. Using XX, use cross validation to optimize the parameters of your moedls



