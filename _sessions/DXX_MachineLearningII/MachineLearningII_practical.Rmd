---
title: "Machine Learning II"
author: "BaselRBootcamp January 2018"
output: html_document
---

```{r, echo = FALSE, fig.align = 'center', out.width = "50%", fig.cap = "Source: https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer"}
knitr::include_graphics("https://uploads.toptal.io/blog/image/443/toptal-blog-image-1407508081138.png")
```


```{r, echo = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, fig.align = 'center')
```


### Slides

- [Here are the introduction slides for this practical on machine learning!](https://therbootcamp.github.io/_sessions/D2S3_MachineLearning/MachineLearning.html)


### Overview

In this practical you'll use the `caret` (**C**lassification **A**nd **RE**gression **T**raining) package to automate many aspects of the machine learning process. By the end of this practical you will know:

1. Which aspects of the machine learning process the `caret` package can automate
2. Select different 'learners' available in the `caret` package
3. Fit parameters of a learner to a dataset
4. Explore learners to understand how they work
5. Use cross validation techniques to estimate the accuracy of a learner

### Glossary and packages

Here are the main functions and packages you'll be using. For more information about the specific models, click on the link in *Additional Details*.

| Algorithm| Function| Package | Additional Details |
|:------|:--------|:----|:----|
|     Regression|    `glm()`| Base R| https://bookdown.org/ndphillips/YaRrr/regression.html#the-linear-model|
|     Fast-and-Frugal decision trees|    `FFTrees()`| FFTrees| https://cran.r-project.org/web/packages/FFTrees/vignettes/guide.html|
|     Decision Trees|    `rpart()`| `rpart` | https://statweb.stanford.edu/~lpekelis/talks/13_datafest_cart_talk.pdf|
| Random Forests | `randomForest()` | `randomForest` | http://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/|


### Packages

For this practical, you will need the following packages:


```{r, eval = FALSE}
install.packages(c("caret", "e1071", "doSNOW", "ipred", "xgboost", "mlbench"))
```

install.packages(c("caret", "e1071", "doSNOW", "ipred", "xgboost"))

```{r}

library(caret)          # for randomForest()
library(xgboost)        # for FFTrees and the heartdisease data
library(tidyverse)      # for dplyr and ggplot2


```


### Examples

- The following examples will take you through all steps of the machine learning process, from creating training and test data, to fitting models, to making predictions. Follow along and try to see how piece of code works!

```{r, eval = FALSE, echo = TRUE}
# -----------------------------------------------
# A step-by-step tutorial for conducting machine learning
# In this tutorial, we'll see how well 3 different models can
#  predict medical data
# ------------------------------------------------

# -----------------------
# Part A:
# Load libraries
# -----------------------

library(caret)          # for randomForest()
library(xgboost)        # for FFTrees and the heartdisease data
library(tidyverse)      # for dplyr and ggplot2


# -----------------------
# Part B: Load titanic dataset
# Look at https://www.kaggle.com/c/titanic/data for a full description
# -----------------------

titanic_full <- read_csv("https://raw.githubusercontent.com/pcsanwald/kaggle-titanic/master/train.csv")


#=================================================================
# Impute Missing Ages
#=================================================================

# Calculte column summaries
titanic_full %>%
  map_dfc(~ sum(is.na(.)))


# Caret supports a number of mechanism for imputing (i.e., 
# predicting) missing values. Leverage bagged decision trees
# to impute missing values for the Age feature.

# First, transform all feature to dummy variables.
dummy_vars <- dummyVars(~ ., data = train %>% select(-survived))
train_dummy <- predict(dummy_vars, newdata = train %>% select(-survived))

# 
train_bagImpute_model <- preProcess(train_dummy, method = "bagImpute")
train_inputed <- as_tibble(predict(train_bagImpute_model, newdata = train_dummy))

#=================================================================
# Split Data
#=================================================================

# Use caret to create a 70/30% split of the training data,
# keeping the proportions of the Survived class label the
# same across splits.
set.seed(100)

train_indices <- createDataPartition(train_inputed$survived,
                                     times = 3,
                                     p = 0.7,
                                     list = FALSE)

titanic_train <- train_inputed %>% 
  slice(train_indices[,1])

titanic_test <- train_inputed %>% 
  slice(-train_indices[,1])


# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(train$survived))
prop.table(table(titanic_train$survived))
prop.table(table(titanic_test$survived))

str(train)


#=================================================================
# Train Model
#=================================================================

# Set up caret to perform 10-fold cross validation repeated 3 
# times and to use a grid search for optimal model hyperparamter
# values.
train.control <- trainControl(method = "repeatedcv",
                              number = 10,              # 10-fold 
                              repeats = 3,              # Do it 3 times
                              search = "grid")          # Find optimal parameters

caret.cv <- train(survived ~ ., 
                  data = titanic_train,
                  method = "xgbTree",
                  trControl = train.control)
```


### Datasets in the mlbench package

We will explore 5 different datasets from the `mlbench` package. This package contains many different datasets that are used throughout the machine learning commmunity to evaluate and compare machine learning models.

```{r, eval = TRUE}
library(mlbench)

data(BostonHousing)
data(DNA)
data(BreastCancer)
data(Soybean)
data(LetterRecognition)
```

```{r datadescriptions, echo = FALSE}
tibble(dataset = "BostonHousing", rows = nrow(BostonHousing), columns = ncol(BostonHousing)) %>%
  add_row(dataset = "DNA", rows = nrow(DNA), columns = ncol(DNA)) %>%
  add_row(dataset = "BreastCancer", rows = nrow(BreastCancer), columns = ncol(BreastCancer)) %>%
  add_row(dataset = "Soybean", rows = nrow(Soybean), columns = ncol(Soybean)) %>%
    add_row(dataset = "LetterRecognition", rows = nrow(LetterRecognition), columns = ncol(LetterRecognition)) %>%
  knitr::kable()
```

### BostonHousing

1. Look at the help menu for your dataset to understand the differnt columns ("features") in the dataset. Make a mental note of which column is the criterion! Is the criterion numeric, binary, or factor?

2. Visually explore the dataset using `summary()`, `str()` and `View()`

3. Using `createPartition`, split the original dataset into a 70\% training set, and a 30\% test set. Make sure to stratify the data according to the criterion.

4. Using `map_dfc`, look for missing data in the training data. Do you find any?

```{r, eval = FALSE}
# Look for missing data in each column of data_train

data_train %>%
  map_dfc(~ sum(is.na(.)))
```

5. If you found mising data, create a model of the missing values using `preProcess`. Then create a new training dataset containing no missing values using `predict()`

6. Create a set of control parameters to setup a 10 fold cross validation.

7. Using `train`, train 3 different models on your data: 
  - Decision tree,
  - Regression (or logistic regression for a binary predictor)
  - Support vector machines

8. Look at the results of the object you just created. Use `names()`, `str()` and `summary()` to see what's in the objects. See if you can access each of the 10 models for each fold of the cross validation.

9. Using XXX, look at the accuracy results, what were the 10 different prediction accuracy levels you got in the 10 fold cross validation? Based on these results, which model do you think will do the best in predicting the true test data?

10. Use XXX to visualise the prediction results for each model.

11. Now, using XX predict the criterion for the *real* test data.

12. Using XX, evaluate the true test accuracy of the model. How did it compare to what you expected from your 10 fold cross validation simulation?

13. Using XX, visualise the final prediction results wi

### Additional options

12. Instead of creating a 70 / 30 training / test split, where 70% of the data were used for training and 30% were used for testing, try repeating your anlyses with a 95 / 5 split. Do your results from before hold?

### Advanced

### Optimising parameters with cross validation.

13. Using XX, use cross validation to optimize the parameters of your moedls



