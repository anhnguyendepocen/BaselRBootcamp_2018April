---
title: "Machine Learning II"
author: "BaselRBootcamp January 2018"
output: html_document
---

```{r, echo = FALSE, fig.align = 'center', out.width = "50%", fig.cap = "Source: https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer"}
knitr::include_graphics("https://uploads.toptal.io/blog/image/443/toptal-blog-image-1407508081138.png")
```


```{r, echo = FALSE}
knitr::opts_chunk$set(comment=NA, fig.width=6, fig.height=6, echo = TRUE, eval = TRUE, fig.align = 'center')
```


### Slides

- [Here are the introduction slides for this practical on machine learning!](https://therbootcamp.github.io/_sessions/D2S3_MachineLearning/MachineLearning.html)


### Overview

In this practical you'll use the `caret` (**C**lassification **A**nd **RE**gression **T**raining) package to automate many aspects of the machine learning process. By the end of this practical you will know:

1. Which aspects of the machine learning process the `caret` package can automate
2. Select different 'learners' available in the `caret` package
3. Fit parameters of a learner to a dataset
4. Explore learners to understand how they work
5. Use cross validation techniques to estimate the accuracy of a learner

### Glossary and packages

Here are the main functions and packages you'll be using. For more information about the specific models, click on the link in *Additional Details*.

| Algorithm| Function| Package | Additional Details |
|:------|:--------|:----|:----|
|     Regression|    `glm()`| Base R| https://bookdown.org/ndphillips/YaRrr/regression.html#the-linear-model|
|     Fast-and-Frugal decision trees|    `FFTrees()`| FFTrees| https://cran.r-project.org/web/packages/FFTrees/vignettes/guide.html|
|     Decision Trees|    `rpart()`| `rpart` | https://statweb.stanford.edu/~lpekelis/talks/13_datafest_cart_talk.pdf|
| Random Forests | `randomForest()` | `randomForest` | http://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/|


### Packages

For this practical, you will need the following packages:


```{r, eval = FALSE}
install.packages(c("caret", "e1071", "doSNOW", "ipred", "xgboost", "mlbench"))
```

install.packages(c("caret", "e1071", "doSNOW", "ipred", "xgboost"))

```{r}

library(caret)          # for randomForest()
library(xgboost)        # for FFTrees and the heartdisease data
library(tidyverse)      # for dplyr and ggplot2


```


### Examples

- The following examples will take you through all steps of the machine learning process, from creating training and test data, to fitting models, to making predictions. Follow along and try to see how piece of code works!

```{r, eval = FALSE, echo = TRUE}
# -----------------------------------------------
# A step-by-step tutorial for conducting machine learning
# In this tutorial, we'll see how well 3 different models can
#  predict medical data
# ------------------------------------------------

# -----------------------
# Part A:
# Load libraries
# -----------------------

library(caret)          # for randomForest()
library(xgboost)        # for FFTrees and the heartdisease data
library(tidyverse)      # for dplyr and ggplot2


# -----------------------
# Part B: Load titanic dataset
# Look at https://www.kaggle.com/c/titanic/data for a full description
# -----------------------

titanic_full <- read_csv("https://raw.githubusercontent.com/pcsanwald/kaggle-titanic/master/train.csv")


#=================================================================
# Impute Missing Ages
#=================================================================

# Caret supports a number of mechanism for imputing (i.e., 
# predicting) missing values. Leverage bagged decision trees
# to impute missing values for the Age feature.

# First, transform all feature to dummy variables.
dummy_vars <- dummyVars(~ ., data = train %>% select(-survived))
train_dummy <- predict(dummy_vars, newdata = train %>% select(-survived))

# 
train_bagImpute_model <- preProcess(train_dummy, method = "bagImpute")
train_inputed <- as_tibble(predict(train_bagImpute_model, newdata = train_dummy))

#=================================================================
# Split Data
#=================================================================

# Use caret to create a 70/30% split of the training data,
# keeping the proportions of the Survived class label the
# same across splits.
set.seed(100)

train_indices <- createDataPartition(train_inputed$survived,
                                     times = 3,
                                     p = 0.7,
                                     list = FALSE)

titanic_train <- train_inputed %>% 
  slice(train_indices[,1])

titanic_test <- train_inputed %>% 
  slice(-train_indices[,1])


# Examine the proportions of the Survived class lable across
# the datasets.
prop.table(table(train$survived))
prop.table(table(titanic_train$survived))
prop.table(table(titanic_test$survived))

str(train)


#=================================================================
# Train Model
#=================================================================

# Set up caret to perform 10-fold cross validation repeated 3 
# times and to use a grid search for optimal model hyperparamter
# values.
train.control <- trainControl(method = "repeatedcv",
                              number = 10,              # 10-fold 
                              repeats = 3,              # Do it 3 times
                              search = "grid")          # Find optimal parameters





caret.cv <- train(survived ~ ., 
                  data = titanic_train,
                  method = "xgbTree",
                  trControl = train.control)
```


### Datasets in the mlbench package

We will explore 5 different datasets from the `mlbench` package. This package contains many different datasets that are used throughout the machine learning commmunity to evaluate and compare machine learning models.

```{r, eval = TRUE}
library(mlbench)

data(BostonHousing)
data(DNA)
data(BreastCancer)
data(Soybean)
data(LetterRecognition)
```

```{r datadescriptions, echo = FALSE}
tibble(dataset = "BostonHousing", rows = nrow(BostonHousing), columns = ncol(BostonHousing)) %>%
  add_row(dataset = "DNA", rows = nrow(DNA), columns = ncol(DNA)) %>%
  add_row(dataset = "BreastCancer", rows = nrow(BreastCancer), columns = ncol(BreastCancer)) %>%
  add_row(dataset = "Soybean", rows = nrow(Soybean), columns = ncol(Soybean)) %>%
    add_row(dataset = "LetterRecognition", rows = nrow(LetterRecognition), columns = ncol(LetterRecognition)) %>%
  knitr::kable()
```


1. Look at the help menu for X to understand the columns

1. Explore the X dataset, do you see any missing data?

2. Impute the missing values with X

3. Using `createPartition`, separate the data into a X% training set X_train and a X% test set called X_test. Make sure to stratify the data so that the training and test sets have relatively equal balances of the criterion.

4. Create a set of control parameters to setup a 10 fold cross validation. Only do 1 repetition.

5. Using `train`, train a decision tree on the data.

6. Look at the results of the object you just created and try to find the original decision tree models

7. Look at the accuracy results, what were the 10 different prediction accuracy levels you got in the 10fold cross validation?

8. Now, using XX predict the criterion for the real test data.

9. Using XX, evaluate the true test accuracy of the model. How did it compare to what you expected from your 10 fold cross validation simulation?

### Optimising parameters with cross validation.

13. Using XX, repeat your processes above to optimize parameter values. 

### Try logistic regression

Now add logisic regression...

### Try random forests

10. Now it's time to try a new model! Instead of a decision tree, try using randomforests with XX. Repeat each of your steps before.

11. Using XX, compare your cross validation results between decision trees and random forests. Which model seems to do better in the cross validations?

12. Let's test your predictions! Predict the test data with your random forest model and evalute it's prediction performance. Do your results coincide with what you expected from your cross validation simulation?

# Using the XXX dataset

Repeat all of your previous steps, but now with the XX dataset. Which model seems to do the best in this dataset?




